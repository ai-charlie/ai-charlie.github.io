# ICLR2013，Word2Vec 词向量扛鼎之作

《Efficient Estimation of Word Representation in Vector Space》https://arxiv.org/abs/1301.3781

“Efficient Estimation of Word Representations in Vector Space”由Tomas Mikolov等人撰写。在自然语言处理领域，传统将单词视为原子单位的方法在一些任务中受限，词向量表示应运而生。本文旨在从大规模数据集中学习高质量词向量，提出两种新模型架构，并在词相似度任务中评估。结果显示新模型计算成本低、准确性高，在句法和语义相似度测试集上表现优异，为自然语言处理应用提供了有力支持。

1. **研究背景**
   - **传统方法局限**：许多当前NLP系统将单词视为原子单位，在一些任务中达到瓶颈，如自动语音识别和机器翻译等领域，简单技术的提升效果有限。
   - **词向量的优势**：随着机器学习技术发展，分布式词表示成为重要概念，神经网络语言模型优于N-gram模型，但现有词向量训练架构存在计算成本高的问题。
2. **模型架构**
   - **前馈神经网络语言模型（NNLM）**：由输入、投影、隐藏和输出层组成，计算投影层和隐藏层之间较复杂，可通过分层softmax等方法降低复杂度，本文使用基于Huffman二叉树的分层softmax。
   - **循环神经网络语言模型（RNNLM）**：无投影层，通过循环矩阵连接隐藏层形成短期记忆，复杂度主要来自隐藏层自身的计算，也可使用分层softmax降低输出层复杂度。
   - **并行训练神经网络**：在DistBelief框架上实现多个模型的并行训练，采用小批量异步梯度下降和Adagrad自适应学习率程序。
   - **新的对数线性模型**
     - **连续词袋模型（CBOW）**：去除非线性隐藏层，共享投影层，将所有单词投影到同一位置，用上下文预测当前词，训练复杂度为$Q = N×D + D×log₂(V)$。
     - **连续跳字模型（Skip - gram）**：与CBOW类似，但基于当前词预测周围词，通过采样调整远距离词的权重，训练复杂度为$Q = C×(D + D×log₂(V))$。
3. **实验结果**
   - **任务描述**：定义包含五种语义和九种句法问题的测试集，评估词向量质量，要求精确匹配答案。
   - **准确性最大化**：使用Google News语料库训练，限制词汇量为100万最频繁词。发现增加向量维度和训练数据量可提高准确性，但存在收益递减，需同时增加两者。同时给出不同模型架构在不同条件下的准确性数据。
   - **模型架构比较**：在相同数据和向量维度下比较不同模型，Skip - gram在语义部分表现最佳，CBOW在句法任务上优于NNLM，且训练速度更快。还与公开可用词向量比较，本文模型表现更好。此外，训练数据量加倍用1个epoch训练的效果与3个epoch相当或更好。
   - **大规模并行训练**：在DistBelief框架上训练模型，给出不同模型在Google News 6B数据集上的训练结果，CBOW和Skip - gram模型在分布式框架下CPU使用率更接近。
   - **微软句子完成挑战**：Skip - gram模型在此任务上虽单独表现不如LSA相似性，但与RNNLMs结合可达到新的最优结果58.9%准确率。
4. **结论与后续工作**
   - 研究表明简单模型架构可训练高质量词向量，计算成本低，能从大规模数据集中计算准确的高维词向量。这些词向量可应用于多种NLP任务，有望推动相关技术发展。
   - 后续发布了单机多线程C++代码和大量命名实体向量，相关工作将在后续论文中发表。

# EMNLP2014，Glove：最出名的词向量训练方法之一

https://aclanthology.org/D14-1162.pdf

《GloVe: Global Vectors for Word Representation》

“GloVe: Global Vectors for Word Representation”由Jeffrey Pennington、Richard Socher和Christopher D. Manning撰写。在自然语言处理中，词向量表示至关重要。以往的全局矩阵分解和局部上下文窗口方法存在缺陷。本文提出GloVe模型，结合两者优势，通过对词共现矩阵非零元素训练，高效利用统计信息。实验表明，该模型在词类比、相似度和命名实体识别任务上表现出色，优于相关模型，为词向量学习提供了新方法。

**研究背景**

- **词向量的重要性**：语义向量空间模型用实值向量表示单词，在信息检索、文档分类等多种应用中作为特征。多数方法以词向量间距离或角度评估其质量，词类比任务成为新的评估方式，推动模型发展。
- **现有方法的局限**
  - **全局矩阵分解方法**：如LSA，虽能有效利用统计信息，但在词类比任务中表现不佳，向量空间结构欠佳。
  - **局部上下文窗口方法**：如skip - gram模型，在类比任务上可能表现较好，但未充分利用语料库统计信息，因在局部上下文窗口训练而非基于全局共现计数。

**GloVe模型介绍**

- **模型构建思路**
  - **基于共现概率**：以词共现概率比值为出发点，如研究“ice”和“steam”与其他词的共现概率比值，发现其能更好区分相关与无关词，表明词向量学习应以共现概率比值为基础。
  - **模型推导过程**：从一般模型$F\left(w_{i}, w_{j}, \tilde{w}_{k}\right)=\frac{P_{i k}}{P_{j k}}$出发，通过限制函数形式、考虑向量空间线性结构、保证对称性等步骤，得到最终模型$w_{i}^{T} \tilde{w}_{j}+b_{i}+\tilde{b}_{j}=\log \left(X_{i j}\right)$，并对其进行对数变换和加权处理，以解决数据稀疏和权重问题，得到加权最小二乘回归模型$J=\sum_{i, j = 1}^{V} f\left(X_{i j}\right)\left(w_{i}^{T} \tilde{w}_{j}+b_{i}+\tilde{b}_{j}-\log X_{i j}\right)^{2}$。
- **与其他模型的关系**：与skip - gram和ivLBL等模型相比，GloVe模型在目标函数上有相似之处，但通过改进距离度量、优化权重函数等，克服了其他模型的缺点，如对长尾分布建模差、计算瓶颈和权重不合理等问题。
- **模型复杂度**：计算复杂度取决于词共现矩阵$X$的非零元素数量，通过假设词共现次数服从幂律分布，得出模型复杂度优于最坏情况$O(|V|^{2})$，且比在线窗口方法$O(|C|)$表现更好。

**实验过程与结果**

- **实验设置**
  - **评估任务**：包括Mikolov等人的词类比任务、多种词相似度任务（如WordSim - 353、MC等）以及CoNLL - 2003命名实体识别任务。
  - **训练数据与参数**：在五个不同大小的语料库上训练，包括维基百科和Gigaword等，设置$x_{max}=100$，$\alpha = 3/4$，采用AdaGrad训练，探索不同向量维度、上下文窗口和语料库大小对结果的影响。
- **实验结果**
  - **词类比任务**：GloVe模型表现显著优于其他基线模型，在不同向量大小和语料库条件下均有优势，且能在大规模语料上有效训练提升性能。
  - **词相似度任务**：在多个数据集上的Spearman等级相关性结果表明，GloVe模型优于其他对比模型。
  - **命名实体识别任务**：在基于CRF的模型中，GloVe模型在除CoNLL测试集外的所有评估指标上均优于其他方法，证明其在下游NLP任务中的有效性。

**模型分析**

- **向量长度和上下文大小**：向量维度大于200时收益递减；语法信息在小且不对称上下文窗口下性能更好，语义信息则在较大窗口下捕获更多。
- **语料库大小**：语法子任务性能随语料库增大单调提升，语义子任务中较小维基百科语料库训练的模型表现更好，可能因维基百科信息更新更及时。
- **运行时间**：运行时间包括构建共现矩阵和训练模型两部分，在特定硬件和参数设置下给出了相应时间消耗，并绘制学习曲线。
- **与word2vec比较**：控制多种参数后，在相同训练时间下，GloVe性能优于word2vec，且能更快达到较好结果。

**研究结论**

- 本文认为基于计数和预测的词向量学习方法在根本上无巨大差异，GloVe模型利用计数数据优势并捕捉线性子结构，在词类比、相似度和命名实体识别任务中表现出色，为词表示学习提供新方法。

# EMNLP2015，Char Embedding 第一篇介绍字符嵌入的论文

https://arxiv.org/abs/1301.3781

《Compositional character models for open vocabulary word representation》

“Efficient Estimation of Word Representations in Vector Space”由Tomas Mikolov等人撰写。随着自然语言处理发展，传统将单词视为原子单位的方法在一些任务中受限，词向量表示成为研究热点。本文旨在从大规模数据集中高效学习高质量词向量，提出两种新模型架构，并在词相似度任务中与已有方法对比。实验表明新模型计算成本低、准确性高，在句法和语义相似度测试集上表现出色，对自然语言处理应用具有重要意义。

**研究背景**

- **传统方法局限**：许多当前NLP系统将单词视为原子单位，在自动语音识别、机器翻译等任务中，简单技术提升效果有限。例如，自动语音识别中高质量转录语音数据量常有限，限制了性能提升。
- **词向量优势与问题**：分布式词表示的神经网络语言模型优于N - gram模型，但现有词向量训练架构计算成本高，限制了其应用和发展。

**模型架构**

- **前馈神经网络语言模型（NNLM）**：由输入、投影、隐藏和输出层组成。输入层用1 - of - V编码，投影层通过共享矩阵投影，隐藏层计算词汇概率分布，导致计算复杂，尤其是投影层和隐藏层之间。可通过分层softmax等方法降低复杂度，本文采用基于Huffman二叉树的分层softmax，根据单词频率分配短二进制码，减少输出单元评估数量。
- **循环神经网络语言模型（RNNLM）**：无投影层，通过循环矩阵连接隐藏层形成短期记忆，可处理更复杂模式。其复杂度主要来自隐藏层自身计算，同样可使用分层softmax降低输出层复杂度。
- **并行训练神经网络**：在DistBelief框架上实现多个模型并行训练，采用小批量异步梯度下降和Adagrad自适应学习率程序，能利用数据中心资源，提高训练效率。
- **新的对数线性模型**
  - **连续词袋模型（CBOW）**：去除非线性隐藏层，共享投影层，将所有单词投影到同一位置（向量平均），用上下文预测当前词。训练复杂度为$Q = N×D + D×log₂(V)$，相比传统模型降低了计算复杂度。
  - **连续跳字模型（Skip - gram）**：与CBOW类似，但基于当前词预测周围词，通过采样调整远距离词的权重。训练复杂度为$Q = C×(D + D×log₂(V))$，增加预测范围可提高词向量质量，但也增加计算复杂度。

**实验结果**

- **任务描述**：构建包含五种语义和九种句法问题的测试集，要求精确匹配答案来评估词向量质量，以全面衡量词向量对语言规律的捕捉能力。
- **准确性最大化**：使用Google News语料库（约6B tokens）训练，限制词汇量为100万最频繁词。实验发现增加向量维度和训练数据量可提高准确性，但存在收益递减，需同时增加两者。同时给出CBOW模型在不同条件下的准确性数据，为模型选择提供参考。
- **模型架构比较**：在相同数据和640维向量下比较不同模型，Skip - gram在语义部分表现最佳，CBOW在句法任务上优于NNLM且训练速度更快。与公开可用词向量比较，本文模型表现更好。此外，训练数据量加倍用1个epoch训练的效果与3个epoch相当或更好，提高了训练效率。
- **大规模并行训练**：在DistBelief框架上训练模型，给出不同模型在Google News 6B数据集上的训练结果，CBOW和Skip - gram模型在分布式框架下CPU使用率更接近，体现了框架对模型训练的影响。
- **微软句子完成挑战**：Skip - gram模型在此任务上虽单独表现不如LSA相似性，但与RNNLMs结合可达到新的最优结果58.9%准确率，展示了模型组合的优势。

**结论与后续工作**

- 研究表明简单模型架构可训练高质量词向量，计算成本低，能从大规模数据集中计算准确的高维词向量，对多种NLP任务有益，有望推动相关技术发展。
- 后续发布了单机多线程C++代码和大量命名实体向量，相关工作将在后续论文中发表，进一步拓展了研究成果的应用和影响。

# EMNLP2014，TextCNN 第一篇 CNN 用于文本分类的文章

《Convolutional Neural Network for Sentence Classification》

https://arxiv.org/abs/1408.5882

NIPS2015，CharTextCNN 第一篇字符级别文本分类模型

“Convolutional Neural Networks for Sentence Classification”由Yoon Kim撰写。在自然语言处理领域，句子分类是重要任务。传统方法有局限，深度学习中卷积神经网络（CNN）在图像和语音领域成果显著，但在NLP中的应用有待深入研究。本文利用预训练词向量训练CNN进行句子分类，实验表明简单CNN效果优异，微调词向量可提升性能，还提出多通道架构改进模型，为句子分类提供了新方法和思路。

- **研究背景与模型架构**
  - **背景**：深度学习在计算机视觉和语音识别取得成果，但在自然语言处理中，利用卷积神经网络（CNN）进行句子分类仍有探索空间。词向量可将单词编码到低维空间，是重要特征提取器，本文在此基础上训练CNN。
  - **模型**：模型架构是Collobert等人CNN架构的变体。句子由词向量拼接表示，卷积操作通过滤波器提取特征，经最大时间池化操作取最大值作为特征，多个滤波器提取的特征传入全连接softmax层输出概率分布。还尝试了多通道架构，用预训练和微调的词向量分别作为通道，滤波器应用于两个通道并相加计算特征，且只通过一个通道反向传播梯度。
  - **正则化**：采用dropout和\(l_{2}\)范数约束对模型进行正则化。dropout在训练时随机丢弃一定比例隐藏单元，测试时缩放权重向量；\(l_{2}\)范数约束在梯度下降后对超出阈值的权重向量进行缩放。
- **数据集与实验设置**
  - **数据集**：在MR、SST - 1、SST - 2等7个基准数据集上测试模型，涵盖情感分析、问题分类等任务，各数据集在类别、长度、规模、词汇量等方面有不同统计特征。
  - **参数与训练**：所有数据集使用修正线性单元，滤波器窗口为3、4、5且各有100个特征图，dropout为0.5，\(l_{2}\)约束为3，小批量为50。通过在SST - 2开发集上网格搜索选择，训练采用随机梯度下降和Adadelta更新规则，并进行早停法。
  - **预训练词向量**：使用在Google News上训练的300维word2vec词向量初始化，未出现的词随机初始化。
  - **模型变体**：实验了CNN - rand（随机初始化词向量）、CNN - static（预训练词向量保持静态）、CNN - non - static（微调预训练词向量）、CNN - multichannel（多通道模型）等变体。
- **结果与讨论**
  - **模型比较结果**：CNN - rand性能不佳，CNN - static表现优异，超过一些复杂深度学习模型，表明预训练词向量是良好的通用特征提取器，微调可进一步提升性能。CNN - multichannel效果有好有坏，需进一步研究正则化微调过程。
  - **多通道与单通道**：多通道架构期望防止过拟合，但结果喜忧参半，可考虑用单通道加额外可训练维度替代。
  - **静态与非静态表示**：非静态通道可针对任务微调，如在情感分析任务中使语义相近词的向量表示更符合任务需求，对未在预训练词向量中的词也能学习到更有意义的表示。
  - **其他观察**：相比Kalchbrenner等人相似架构模型，本文CNN因容量大（多滤波器宽度和特征图）性能更好；dropout是有效正则化方法，可使用较大网络；初始化未在word2vec中的词向量时，使其方差与预训练词向量相同可提升性能；word2vec性能优于Collobert等人在Wikipedia上训练的词向量；Adadelta与Adagrad效果相似但更快。
- **研究结论**：基于word2vec的简单CNN在句子分类任务中表现出色，证明了无监督预训练词向量在NLP深度学习中的重要性。


# EACL2017，FastText 细粒度的文本分类

《Bag of Tricks for Efficient Text Classification》

https://arxiv.org/abs/1607.01759

LSTM_NMT 使用 LSTM 解决机器翻译问题

“Bag of Tricks for Efficient Text Classification”由Armand Joulin等人撰写。在自然语言处理中，文本分类是重要任务，神经网络模型虽性能好但训练和测试慢，线性分类器有潜力但存在局限。本文提出fastText模型，通过一系列设计实现高效文本分类。实验表明，fastText在准确性上与深度学习分类器相当，训练和评估速度却快得多，为大规模文本分类提供了有效方法。

- **研究背景与模型架构**
  - **背景**：文本分类在自然语言处理中应用广泛，但基于神经网络的模型在大规模数据集上训练和测试速度慢，而线性分类器虽简单但在特定条件下性能不错且有扩展性，本文在此背景下探索高效文本分类方法。
  - **模型**：采用类似词袋模型的架构，将句子表示为词袋并训练线性分类器，通过权重矩阵将词表示平均为文本表示再输入分类器，用softmax计算类别概率分布，采用异步训练和随机梯度下降及线性衰减学习率。
  - **分层softmax**：当类别数量大时，为降低计算复杂度使用基于Huffman编码树的分层softmax，训练和测试时计算复杂度降为\(O(h log _{2}(k))\)，还可扩展计算\(T - top\)目标。
  - **N - gram特征**：使用词袋模型无法考虑词序，故采用N - gram特征作为补充以捕获局部词序信息，通过哈希技巧高效映射N - gram，在实践中效果好且计算成本低。
- **实验过程与结果**
  - **情感分析**：在多个情感分析数据集上与多种方法比较，如词袋模型、卷积神经网络等。fastText使用10个隐藏单元和5个训练周期，添加二元语法可提高1 - 4%性能，总体准确性略优于部分模型，稍逊于VDCNN。训练时间上，相比基于卷积的方法在CPU上快很多，与Tang等人基于循环网络的方法相比也有极大速度优势。
  - **标签预测**：在YFCC100M数据集上测试模型扩展性，与频率基线和Tagspace模型比较。fastText运行5个周期，添加二元语法可显著提高准确性，在测试时速度比Tagspace快很多，尤其是类别数量大时，能更快获得高质量模型。
- **研究结论与意义**：本文提出的fastText是一种简单有效的文本分类基线方法，在多个任务中性能与深度学习方法相当但速度更快。其代码将发布，有助于推动相关研究，为文本分类提供了新的实用方案。


# ICLR2015，Bahdanau_NMT 第一篇介绍 attention 的论文

《Neural Machine Translation by Jointly Learning to Align and Translate》

https://arxiv.org/abs/1409.0473

“Neural Machine Translation by Jointly Learning to Align and Translate”由Dzmitry Bahdanau等人撰写。传统的神经机器翻译采用编码器 - 解码器架构，将源句子编码为固定长度向量进行解码，但在处理长句子时存在瓶颈。本文提出一种新架构，通过让模型在生成每个目标词时自动搜索源句子的相关部分，实现了更好的翻译性能，尤其在长句子翻译上表现突出，在英法翻译任务中取得了与现有基于短语的系统相当的结果。

- **研究背景**
  - 神经机器翻译是一种新兴的机器翻译方法，与传统的统计机器翻译不同，它旨在构建一个单一的神经网络来最大化翻译性能。大多数现有的神经机器翻译模型属于编码器 - 解码器家族，但将源句子编码为固定长度向量可能会限制其对长句子的处理能力。
- **模型架构**
  - **编码器**：采用双向循环神经网络（BiRNN）对输入句子进行编码。BiRNN由前向和后向RNN组成，分别按顺序和逆序读取输入序列，得到每个单词的包含前后文信息的注释向量，这些注释向量将用于解码器计算上下文向量。
  - **解码器**：在生成每个目标词时，根据之前的隐藏状态、上一个生成的目标词和一个特定的上下文向量来计算条件概率。这个上下文向量是通过对编码器生成的注释向量进行加权求和得到的，权重由一个对齐模型计算得出，该对齐模型基于解码器的隐藏状态和源句子的注释向量来评估它们之间的匹配程度。这种机制使得模型能够关注源句子的不同部分，而不是将所有信息压缩到一个固定长度向量中。
- **实验设置**
  - **数据集**：使用ACL WMT ’14提供的英语 - 法语双语平行语料库，经过数据选择和预处理后，使用每种语言的30,000个最频繁单词的短列表进行模型训练，并将数据划分为训练集、开发集和测试集。
  - **模型训练**：训练了两种类型的模型，即传统的RNN Encoder - Decoder（RNNencdec）和提出的RNNsearch模型，每种模型分别在不同长度的句子上进行训练。使用小批量随机梯度下降（SGD）算法和Adadelta自适应学习率方法进行训练，每个小批量包含80个句子。
- **实验结果**
  - **定量结果**：在所有情况下，RNNsearch模型的性能都优于传统的RNNencdec模型。特别是在处理长句子时，RNNsearch表现出更强的鲁棒性，其性能与传统的基于短语的翻译系统（Moses）相当，而Moses在训练时使用了额外的单语语料库。
  - **定性分析**
    - **对齐分析**：通过可视化对齐权重，发现模型生成的软对齐在英语和法语单词之间的对齐大多是单调的，但也能处理一些非平凡的、非单调的对齐情况，并且能够自然地处理源和目标短语长度不同的问题。
    - **长句子翻译**：通过具体的长句子翻译示例，展示了RNNsearch模型在保留源句子完整意义方面优于RNNencdec模型，进一步证实了新架构在长句子翻译上的优势。
- **研究结论**：提出的联合学习对齐和翻译的架构有效地解决了传统编码器 - 解码器模型在长句子翻译中的问题，通过在英法翻译任务中的良好表现，证明了该方法的有效性和潜力，为神经机器翻译的发展提供了新的方向，但未来仍需解决处理未知或罕见单词的挑战。

# NAACL2016，Han_Attention attention 用于文本分类

《Hierarchical Attention Networks for Document》

https://aclanthology.org/N16-1174.pdf

“Hierarchical Attention Networks for Document Classification”由Zichao Yang等人撰写。在自然语言处理中，文本分类是基础任务，传统方法用稀疏词汇特征和线性模型或核方法，深度学习方法虽有成效但仍有改进空间。本文提出层次注意力网络（HAN），利用文档的层次结构和两级注意力机制构建文档表示。实验表明，该模型在六个大规模文本分类任务上显著优于先前方法，注意力层可视化显示其能有效选取重要信息，为文档分类提供了新的有效方法。

- **研究背景**
  - 文本分类是自然语言处理的基本任务，传统方法用稀疏词汇特征和线性模型或核方法，深度学习方法如卷积神经网络和基于长短期记忆的循环神经网络也被应用，但本文假设结合文档结构知识可获得更好表示。
- **层次注意力网络（HAN）**
  - **整体架构**：由单词序列编码器、单词级注意力层、句子编码器和句子级注意力层组成。
  - **GRU 序列编码器**：基于门控循环单元（GRU），通过更新门和重置门控制信息更新，计算序列状态。
  - **层次注意力机制**
    - **单词编码与注意力**：先将单词嵌入向量，用双向 GRU 得到包含上下文信息的注释，再通过注意力机制提取重要单词，加权求和得到句子向量。
    - **句子编码与注意力**：用双向 GRU 编码句子向量，得到句子注释，再次用注意力机制选取重要句子，加权求和得到文档向量。
  - **文档分类**：用文档向量作为特征，通过 softmax 函数进行分类，以负对数似然为训练损失。
- **实验**
  - **数据集**：在六个大规模文档分类数据集上评估，包括情感分析和主题分类任务，按 80%、10%、10%比例划分为训练集、验证集和测试集。
  - **基线方法**：与线性方法、支持向量机、神经网络方法（如 LSTM、CNN 等）等基线方法比较。
  - **模型配置与训练**：对文档进行句子分割和分词，用训练好的 word2vec 模型获取词嵌入，在验证集上调优超参数，采用小批量随机梯度下降训练。
  - **结果与分析**：HAN 模型（HN - ATT）在所有数据集上性能最佳，相比未利用文档结构的神经网络方法和仅探索层次结构的方法有明显优势，且在不同数据规模和任务类型上均有提升。同时，可视化和分析表明模型能捕捉上下文相关的单词重要性，并有效选取重要句子和单词。
- **相关工作与结论**
  - 介绍了神经网络在文本分类、注意力机制在其他领域的应用等相关工作，本文的层次注意力机制具有创新性。
  - 提出的 HAN 模型通过聚合重要单词和句子向量构建文档向量，实验结果良好，可视化证明了其有效性。

# Coling2018，SGM 第一篇使用序列生成做多标签文本分类

《SGM: Sequence Generation Model for Multi-label Classification》

https://arxiv.org/abs/1806.04822

“SGM: Sequence Generation Model for Multi - Label Classification”由Pengcheng Yang等人撰写。多标签分类在自然语言处理中至关重要，但现有方法存在不足。本文提出将多标签分类视为序列生成问题，利用具有新颖解码器结构的序列生成模型解决该问题。实验表明，该方法显著优于已有方法，能有效捕捉标签间相关性并自动选择关键信息。

- **研究背景**
  - 多标签分类在自然语言处理中应用广泛，但具有挑战性，因为标签之间存在相关性，且文本不同部分对不同标签的贡献不同，现有方法常忽略这些因素。
- **提出的方法**
  - **模型概述**：将多标签分类任务建模为寻找最优标签序列，对样本标签序列按频率排序并添加起始和结束符号。模型包括编码器、解码器和注意力机制，编码器将文本序列编码为隐藏状态，注意力机制生成上下文向量，解码器基于上下文向量、上一隐藏状态和标签嵌入生成当前隐藏状态和标签概率分布。
  - **序列生成模型**：编码器使用双向LSTM对文本序列编码，注意力机制根据解码器当前隐藏状态为文本单词分配权重并生成上下文向量，解码器利用LSTM和相关输入计算隐藏状态和标签概率，训练采用交叉熵损失函数，推理使用束搜索算法。
  - **全局嵌入**：为缓解标签预测中的曝光偏差问题，提出全局嵌入。通过考虑标签概率分布中的所有信息，将原始嵌入和加权平均嵌入组合，利用转换门自动确定组合因子，使模型能更准确地预测标签序列。
- **实验**
  - **数据集与指标**：在RCV1 - V2和AAPD数据集上评估，采用汉明损失和微\(F_{1}\)分数等作为评估指标。
  - **实验设置**：对不同数据集设置相应的词汇表、词嵌入大小、隐藏层大小等参数，使用Adam优化器训练模型，并采用dropout和梯度裁剪防止过拟合。
  - **基线方法**：与二元相关性（BR）、分类器链（CC）、标签幂集（LP）、CNN、CNN - RNN等基线方法比较。
  - **结果与分析**：在两个数据集上，提出的SGM模型及其带全局嵌入的版本（SGM + GE）在主要评估指标上均优于所有基线方法。全局嵌入能显著提高模型性能，通过实验探索了其对模型的影响；还分析了掩码和标签排序操作的作用、模型在不同标签序列长度下的性能、注意力机制的可视化效果以及通过案例展示了模型捕捉标签相关性的能力。
- **相关工作与结论**：介绍了多标签分类的四类主要方法，包括问题转换、算法适应、集成和神经网络模型。本文提出的方法在性能上有显著优势，但对于样本具有大量标签的情况仍需进一步探索更有效的解决方案。
