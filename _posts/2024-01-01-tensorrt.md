# TensorRT

TensorRTæ˜¯å¯ä»¥åœ¨NVIDIAå„ç§GPUç¡¬ä»¶å¹³å°ä¸‹è¿è¡Œçš„ä¸€ä¸ªC++æ¨ç†æ¡†æ¶ã€‚æˆ‘ä»¬åˆ©ç”¨Pytorchã€TFæˆ–è€…å…¶ä»–æ¡†æ¶è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¯ä»¥è½¬åŒ–ä¸ºTensorRTçš„æ ¼å¼ï¼Œç„¶ååˆ©ç”¨TensorRTæ¨ç†å¼•æ“å»è¿è¡Œæˆ‘ä»¬è¿™ä¸ªæ¨¡å‹ï¼Œä»è€Œæå‡è¿™ä¸ªæ¨¡å‹åœ¨è‹±ä¼Ÿè¾¾GPUä¸Šè¿è¡Œçš„é€Ÿåº¦ã€‚

åœ¨GPUæœåŠ¡å™¨ä¸Šéƒ¨ç½²çš„è¯ï¼ŒTensorRTæ˜¯é¦–é€‰ï¼

**TensorRTæ˜¯ç¡¬ä»¶ç›¸å…³çš„ï¼Œ**ä¸åŒçš„ç¡¬ä»¶ä¸åŒçš„ä¼˜åŒ–ã€‚TensorRTæ˜¯**åŠå¼€æº**çš„ï¼Œé™¤**æ ¸å¿ƒéƒ¨åˆ†å¤–**åŸºæœ¬éƒ½å¼€æºäº†ã€‚

## TensorRTæ¨¡å‹è½¬æ¢

1. `TF-TRT`ï¼ŒTensorRTé›†æˆåœ¨TensorFlowä¸­
2. `ONNX2TensorRT`ï¼Œå³ONNXè½¬æ¢trtçš„å·¥å…·

   ```
   # Or use trtexec to convert ONNX to TensorRT engine
   /usr/src/tensorrt/bin/trtexec --onnx=yolov7-tiny.onnx --saveEngine=yolov7-tiny-nms.trt --fp16
   ```
3. æ‰‹åŠ¨æ„é€ æ¨¡å‹ç»“æ„ï¼Œç„¶åæ‰‹åŠ¨å°†æƒé‡ä¿¡æ¯æŒªè¿‡å»ï¼Œéå¸¸çµæ´»ä½†æ˜¯æ—¶é—´æˆæœ¬ç•¥é«˜ï¼Œæœ‰å¤§ä½¬å·²ç»å°è¯•è¿‡äº†.

## Project

[https://github.com/wang-xinyu/pytorchx]() Implement popular deep learning networks in pytorch, used by

## PyTorchä¸TensorRT

æ¨¡å‹è½¬æ¢æ–¹å¼ï¼š**PyTorch â†’ ONNX â†’ TensorRT C++API**

- torch2trt  https://github.com/NVIDIA-AI-IOT/torch2trt
- torch2trt_dynamic  https://github.com/grimoire/torch2trt_dynamic
- TRTorch  https://github.com/pytorch/TensorRTï¼Œpytorchå®˜æ–¹æ–‡æ¡£

## ç†è§£TensorRT Runtimes

åœ¨TensorRT Runtimeç¯å¢ƒä¸­è¿è¡Œæ¨¡å‹ï¼Œå°±æ˜¯ç›´æ¥ä½¿ç”¨TensorRTï¼›

[TensorRT/5. Understanding TensorRT Runtimes.ipynb at main Â· NVIDIA/TensorRT](https://github.com/NVIDIA/TensorRT/blob/main/quickstart/IntroNotebooks/5.%20Understanding%20TensorRT%20Runtimes.ipynb)

**Python API**

[TensorRT/4. Using PyTorch through ONNX.ipynb at 87f3394404ff9f9ec92c906cd4c39b5562aea42e Â· NVIDIA/TensorRT](https://github.com/NVIDIA/TensorRT/blob/87f3394404ff9f9ec92c906cd4c39b5562aea42e/quickstart/IntroNotebooks/4.%20Using%20PyTorch%20through%20ONNX.ipynb)

**C++ API** [https://developer.nvidia.com/blog/speed-up-inference-tensorrt/](https://developer.nvidia.com/blog/speed-up-inference-tensorrt/)

**Tensorflow/TF-TRT Runtime: (Tensorflow Only) æ“ä½œç®€å•ï¼ŒåŠ é€Ÿæ•ˆæœä¸å¥½**

**TRITON Inference Server** å®Œç¾æ”¯æŒTensorRTï¼Œç”¨åœ¨ç”Ÿäº§ç¯å¢ƒ[https://github.com/NVIDIA/TensorRT/tree/main/quickstart/deploy_to_triton](https://github.com/NVIDIA/TensorRT/tree/main/quickstart/deploy_to_triton)

[Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server)

### Nvidia å®˜æ–¹é•œåƒ

nvidiaå®˜æ–¹æä¾›äº†tensorrtä»¥åŠåŒ…å«tensorrtçš„æµè¡Œæ·±åº¦å­¦ä¹ ç¯å¢ƒçš„é•œåƒï¼Œä¾‹å¦‚tensorflowã€paddlepaddleã€torch

æ¨èpytorché•œåƒ

[PyTorch Release Notes :: NVIDIA Deep Learning Frameworks Documentation](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel_22-06.html#rel_22-06)

æŸ¥çœ‹nvidiaçš„pytorché•œåƒåŒ…å«çš„åº“ç‰ˆæœ¬

[NVIDIA TensorRT](https://docs.nvidia.com/deeplearning/tensorrt/index.html)

[Git History](https://github.githistory.xyz/NVIDIA/trt-samples-for-hackathon-cn/blob/master/cookbook/README.md)

<aside>
ğŸ’¡ Tensorrt ä»…æ”¯æŒåœ¨GPUç¯å¢ƒä¸‹ä½¿ç”¨

# Tensorrt C++éƒ¨ç½²

## ç¯å¢ƒé…ç½®

### Dockerfile

ç”±å®˜æ–¹https://github.com/NVIDIA/TensorRT æä¾›çš„Ubuntu18.04.Dockerfileä¿®æ”¹è€Œæ¥ï¼Œå‚è€ƒ [https://github.com/NVIDIA/cuda-repo-management/issues/4](https://github.com/NVIDIA/cuda-repo-management/issues/4)è§£å†³GPG errorï¼Œå‚è€ƒå®˜æ–¹å®‰è£…æ–‡æ¡£ [https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-debian](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-debian) æ„å»º

- æ„å»ºåŸºç¡€tensorrtç‰ˆæœ¬

  ```docker
  # æŸ¥çœ‹é€‚é…tensorrtçš„cudaç‰ˆæœ¬
  # https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/
  ARG CUDA_VERSION=10.2
  #ARG CUDA_VERSION=11.4.2
  ARG OS_VERSION=18.04

  # åŸºç¡€é•œåƒ
  FROM nvidia/cuda:${CUDA_VERSION}-cudnn8-devel-ubuntu${OS_VERSION}
  LABEL maintainer="zhanglq"

  ENV TRT_VERSION 8.4.1.3
  # ENV TRT_VERSION 8.2.5.1
  SHELL ["/bin/bash", "-c"]

  # Setup user account
  # ARG uid=1000
  # ARG gid=1000
  # RUN groupadd -r -f -g ${gid} trtuser && useradd -o -r -l -u ${uid} -g ${gid} -ms /bin/bash trtuser
  # RUN usermod -aG sudo trtuser
  # RUN echo 'trtuser:nvidia' | chpasswd
  # RUN mkdir -p /workspace && chown trtuser /workspace

  # Repair the GPG error
  # https://github.com/NVIDIA/cuda-repo-management/issues/4
  RUN rm /etc/apt/sources.list.d/cuda.list
  RUN rm /etc/apt/sources.list.d/nvidia-ml.list
  RUN apt-key del 7fa2af80
  RUN apt-get update && apt-get install -y --no-install-recommends wget
  RUN wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-keyring_1.0-1_all.deb
  RUN dpkg -i cuda-keyring_1.0-1_all.deb

  # Install requried libraries
  RUN apt-get update && apt-get install -y software-properties-common
  RUN add-apt-repository ppa:ubuntu-toolchain-r/test
  RUN apt-get update && apt-get install -y --no-install-recommends \
      libcurl4-openssl-dev \
      zlib1g-dev \
      git \
      pkg-config \
      sudo \
      ssh \
      libssl-dev \
      pbzip2 \
      pv \
      bzip2 \
      unzip \
      devscripts \
      lintian \
      fakeroot \
      dh-make \
      build-essential

  # Install python3
  # è½¯è¿æ¥ ä½¿pip3å‘½ä»¤ç­‰äºpip
  RUN apt-get install -y --no-install-recommends \
        python3 \
        python3-pip \
        python3-dev \
        python3-wheel &&\
      cd /usr/local/bin &&\
      ln -s /usr/bin/python3 python &&\
      ln -s /usr/bin/pip3 pip;

  # Install TensorRT
  RUN if [ "${CUDA_VERSION}" = "10.2" ] ; then \
      v="${TRT_VERSION%.*}-1+cuda${CUDA_VERSION}" &&\
      apt-get update &&\
      sudo apt-get install libnvinfer8=${v} libnvonnxparsers8=${v} libnvparsers8=${v} libnvinfer-plugin8=${v} \
          libnvinfer-dev=${v} libnvonnxparsers-dev=${v} libnvparsers-dev=${v} libnvinfer-plugin-dev=${v} \
          python3-libnvinfer=${v}; \
  else \
      v="${TRT_VERSION%.*}-1+cuda${CUDA_VERSION%.*}" &&\
      apt-get update &&\
      apt-get install libnvinfer8=${v} libnvonnxparsers8=${v} libnvparsers8=${v} libnvinfer-plugin8=${v} \
          libnvinfer-dev=${v} libnvonnxparsers-dev=${v} libnvparsers-dev=${v} libnvinfer-plugin-dev=${v} \
          python3-libnvinfer=${v}; \
  fi

  # é˜»æ­¢tensorrtè‡ªåŠ¨å‡çº§
  RUN apt-mark hold libnvinfer8 libnvonnxparsers8 libnvparsers8 libnvinfer-plugin8 libnvinfer-dev libnvonnxparsers-dev libnvparsers-dev libnvinfer-plugin-dev python3-libnvinfer

  # å¦‚æœæƒ³è¦å‡çº§æœ€æ–°ç‰ˆæœ¬çš„tensorrt
  # sudo apt-mark unhold libnvinfer8 libnvonnxparsers8 libnvparsers8 libnvinfer-plugin8 libnvinfer-dev libnvonnxparsers-dev libnvparsers-dev libnvinfer-plugin-dev python3-libnvinfer

  # Install PyPI packages
  RUN pip3 install --upgrade pip
  RUN pip3 config set global.index-url https://pypi.douban.com/simple/
  RUN pip3 install setuptools>=41.0.0

  # Install jupyter
  RUN pip3 install jupyter jupyterlab
  # Workaround to remove numpy installed with tensorflow
  RUN pip3 install --upgrade numpy
  RUN pip3 install ipykernal
  RUN pip3 install ipywidgets --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host=files.pythonhosted.org
  RUN jupyter nbextension enable --py widgetsnbextension

  # Install Cmake
  RUN cd /tmp && \
      wget https://github.com/Kitware/CMake/releases/download/v3.14.4/cmake-3.14.4-Linux-x86_64.sh && \
      chmod +x cmake-3.14.4-Linux-x86_64.sh && \
      ./cmake-3.14.4-Linux-x86_64.sh --prefix=/usr/local --exclude-subdir --skip-license && \
      rm ./cmake-3.14.4-Linux-x86_64.sh

  # # Download NGC client
  # RUN cd /usr/local/bin && wget https://ngc.nvidia.com/downloads/ngccli_cat_linux.zip && unzip ngccli_cat_linux.zip && chmod u+x ngc && rm ngccli_cat_linux.zip ngc.md5 && echo "no-apikey\nascii\n" | ngc config set

  # Set environment and working directory
  ENV TRT_LIBPATH /usr/lib/x86_64-linux-gnu
  ENV TRT_OSSPATH /workspace/TensorRT
  ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:${TRT_OSSPATH}/build/out:${TRT_LIBPATH}"
  WORKDIR /workspace

  # USER trtuser
  RUN ["/bin/bash"]
  ```
- è‡ªåŠ¨åŒ–æ„å»ºè„šæœ¬

  ```bash
  #!/usr/bin/env bash

  arg_dockerfile=Dockerfile
  arg_imagename=tensorrt:trt8.4.1-cu10.2-cudnn8-ubuntu1804-py36
  arg_help=0

  while [[ "$#" -gt 0 ]]; do case $1 in
    -f) arg_dockerfile="$2"; shift;;
    -t) arg_imagename="$2"; shift;;
    -h|--help) arg_help=1;;
    *) echo "Unknown parameter passed: $1"; echo "For help type: $0 --help"; exit 1;
  esac; shift; done

  if [ "$arg_help" -eq "1" ]; then
      echo "Usage: $0 [options]"
      echo " --help or -h         : Print this help menu."
      echo " -f   <dockerfile> : Docker file to use for build."
      echo " -t   <imagename>  : Image name for the generated container."
      exit;
  fi

  docker_args="-f $arg_dockerfile -t $arg_imagename ."

  echo "Building container:"
  echo "> docker build $docker_args"
  docker build $docker_args
  ```
- äºŒæ¬¡æ„å»ºåŒ…å«torchå’Œtfçš„ç¯å¢ƒ

  ```docker
  from tensorrt:trt8.4.1_cu10.2-cudnn8
  LABEL maintainer="zhanglq"

  RUN pip3 install onnx==1.10.2; python_version<"3.10"
  RUN pip3 install onnx==1.12.0; python_version=="3.10"
  RUN pip3 install onnxruntime==1.8.1; python_version<"3.10"
  RUn pip3 install onnxruntime==1.12.1; python_version=="3.10"
  RUN pip3 install onnx-graphsurgeon --extra-index-url https://pypi.ngc.nvidia.com

  # Install Tensorflow
  RUN pip3 tensorflow-gpu==2.9.1; (platform_machine=="x86_64" and sys.platform=="linux")

  RUN pip3 install nvidia-pyindex --index-url https://pypi.ngc.nvidia.com

  RUN pip3 install polygraphy \
      pandas \
      seaborn \
      scikit-image \
      scikit-learn \
      cuda-python \
      opencv-python-headless==3.4.16.59 \
      colored \
      scipy \
      tqdm \
      tf2onnx \
      Pillow

  # Install Pytorch
  # ç¨³å®šç‰ˆæœ¬
  # RUN pip3 install --pre torch torchvision torchaudio
  # æœ€æ–°çš„torch
  RUN pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu102
  ```
- è‡ªåŠ¨åŒ–æ„å»ºå®¹å™¨è„šæœ¬

  ```bash
  #!/usr/bin/env bash

  arg_dockerfile=Dockerfile
  arg_imagename=tensorrt:trt8.4.1-cu10.2-cudnn8-ubuntu1804-py36
  arg_help=0

  while [[ "$#" -gt 0 ]]; do case $1 in
    -f) arg_dockerfile="$2"; shift;;
    -t) arg_imagename="$2"; shift;;
    -h|--help) arg_help=1;;
    *) echo "Unknown parameter passed: $1"; echo "For help type: $0 --help"; exit 1;
  esac; shift; done

  if [ "$arg_help" -eq "1" ]; then
      echo "Usage: $0 [options]"
      echo " --help or -h         : Print this help menu."
      echo " -f   <dockerfile> : Docker file to use for build."
      echo " -t   <imagename>  : Image name for the generated container."
      exit;
  fi

  docker_args="-f $arg_dockerfile -t $arg_imagename ."

  echo "Building container:"
  echo "> docker build $docker_args"
  docker build $docker_args
  ```
- è¿è¡Œå®¹å™¨è„šæœ¬

  arg_jupyter=0 åˆ™ä¸è¿è¡Œjupyter

  ```bash
  #!/usr/bin/env bash

  # arg_tag=nvcr.io/nvidia/pytorch:21.10-py3
  # arg_tag=nvidia/cuda:10.2-cudnn8-devel-ubuntu18.04
  arg_tag=nvidia/tensorrt:trt8.4.1-cu10.2-cudnn8-ubuntu1804-py36-torch-tf2
  #arg_tag=nvidia/tensorrt:trt8.4.1-cu11.4.2-cudnn8-ubuntu1804-py36-torch-tf2
  arg_gpus=all
  arg_jupyter=8887
  arg_help=0

  while [[ "$#" -gt 0 ]]; do case $1 in
    --tag) arg_tag="$2"; shift;;
    --gpus) arg_gpus="$2"; shift;;
    --jupyter) arg_jupyter="$2"; shift;;
    -h|--help) arg_help=1;;
    *) echo "Unknown parameter passed: $1"; echo "For help type: $0 --help"; exit 1;
  esac; shift; done

  if [ "$arg_help" -eq "1" ]; then
      echo "Usage: $0 [options]"
      echo " --help or -h         : Print this help menu."
      echo " --tag     <imagetag> : Image name for generated container."
      echo " --gpus    <number>   : Number of GPUs visible in container. Set 'none' to disable, and 'all' to make all visible."
      echo " --jupyter <port>     : Launch Jupyter notebook using the specified port number."
      exit;
  fi

  extra_args=""
  if [ "$arg_gpus" != "none" ]; then
      extra_args="$extra_args --gpus $arg_gpus"
  fi

  if [ "$arg_jupyter" -ne "0" ]; then
      extra_args+=" -p $arg_jupyter:$arg_jupyter"
  fi

  docker_args="$extra_args --shm-size=8gb --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -v ${PWD}:/workspace/code --rm -it $arg_tag"

  if [ "$arg_jupyter" -ne "0" ]; then
      docker_args+=" jupyter-lab --port=$arg_jupyter --no-browser --ip 0.0.0.0 --allow-root"
  fi

  echo "Launching container:"
  echo "> docker run $docker_args"
  docker run $docker_args
  ```

# TensorRTç‰ˆæœ¬é€‚é…

To build the TensorRT-OSS components, you will first need the following software packages.

**TensorRT GA build675**

- [TensorRT](https://developer.nvidia.com/nvidia-tensorrt-download) v8.4.3.1

**System Packages**

- [CUDA](https://developer.nvidia.com/cuda-toolkit)
  - Recommended versions:
  - cuda-11.6.x + cuDNN-8.4
  - cuda-10.2 + cuDNN-8.4
- [GNU make](https://ftp.gnu.org/gnu/make/) >= v4.1
- [cmake](https://github.com/Kitware/CMake/releases) >= v3.13
- [python](https://www.python.org/downloads/) >= v3.6.9
- [pip](https://pypi.org/project/pip/#history) >= v19.0
- Essential utilities
  - [git](https://git-scm.com/downloads), [pkg-config](https://www.freedesktop.org/wiki/Software/pkg-config/), [wget](https://www.gnu.org/software/wget/faq.html#download)

**Optional Packages**

- Containerized build

  - [Docker](https://docs.docker.com/install/) >= 19.03
  - [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-docker)
- Toolchains and SDKs

  - (Cross compilation for Jetson platform) [NVIDIA JetPack](https://developer.nvidia.com/embedded/jetpack) >= 5.0 (current support only for TensorRT 8.4.0)
  - (For Windows builds) [Visual Studio](https://visualstudio.microsoft.com/vs/older-downloads/) 2017 Community or Enterprise edition
  - (Cross compilation for QNX platform) [QNX Toolchain](https://blackberry.qnx.com/en)
- PyPI packages (for demo applications/tests)

  - [onnx](https://pypi.org/project/onnx/) 1.9.0
  - [onnxruntime](https://pypi.org/project/onnxruntime/) 1.8.0
  - [tensorflow-gpu](https://pypi.org/project/tensorflow/) >= 2.5.1
  - [Pillow](https://pypi.org/project/Pillow/) >= 9.0.1
  - [pycuda](https://pypi.org/project/pycuda/) < 2021.1
  - [numpy](https://pypi.org/project/numpy/)
  - [pytest](https://pypi.org/project/pytest/)
- Code formatting tools (for contributors)

  - [Clang-format](https://clang.llvm.org/docs/ClangFormat.html)
  - [Git-clang-format](https://github.com/llvm-mirror/clang/blob/master/tools/clang-format/git-clang-format)

  > NOTE: onnx-tensorrt, cub, and protobuf packages are downloaded along with TensorRT OSS, and not required to be installed.
  >

## Torchè½¬ONNX

å°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼ï¼Œä¸€èˆ¬å¯æŒ‰å¯¼å…¥å¿…è¦åº“ã€åŠ è½½PyTorchæ¨¡å‹ã€è®¾ç½®è¾“å…¥å’Œå¯¼å‡ºç­‰æ­¥éª¤è¿›è¡Œï¼Œä»¥ä¸‹æ˜¯å…·ä½“æ­¥éª¤å’Œä»£ç ç¤ºä¾‹ï¼š

### 1. å¯¼å…¥å¿…è¦çš„åº“

é¦–å…ˆï¼Œéœ€è¦å¯¼å…¥PyTorchå’ŒONNXç›¸å…³çš„åº“ã€‚

```python
import torch
import torch.onnx
```

### 2. åŠ è½½PyTorchæ¨¡å‹

å‡è®¾å·²ç»è®­ç»ƒå¥½äº†ä¸€ä¸ªPyTorchæ¨¡å‹ï¼Œå¹¶ä¿å­˜ä¸º `.pth`æ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç åŠ è½½æ¨¡å‹ã€‚è¿™é‡Œä»¥ä¸€ä¸ªç®€å•çš„å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ä¸ºä¾‹ï¼š

```python
# å®šä¹‰æ¨¡å‹ç±»
class SimpleCNN(torch.nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        self.relu = torch.nn.ReLU()
        self.fc1 = torch.nn.Linear(16 * 32 * 32, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        return x

# åŠ è½½æ¨¡å‹æƒé‡
model = SimpleCNN()
model.load_state_dict(torch.load('model.pth'))
model.eval()
```

### 3. è®¾ç½®è¾“å…¥ç¤ºä¾‹å’ŒåŠ¨æ€è½´

åˆ›å»ºä¸€ä¸ªè¾“å…¥ç¤ºä¾‹ï¼Œç”¨äºæŒ‡å®šæ¨¡å‹çš„è¾“å…¥å½¢çŠ¶ã€‚è¿™ä¸ªè¾“å…¥ç¤ºä¾‹çš„å½¢çŠ¶åº”è¯¥ä¸æ¨¡å‹æœŸæœ›çš„è¾“å…¥å½¢çŠ¶ä¸€è‡´ã€‚åŒæ—¶ï¼Œå¯ä»¥æŒ‡å®šåŠ¨æ€è½´ï¼Œä»¥ä¾¿åœ¨ONNXæ¨¡å‹ä¸­æ”¯æŒä¸åŒå¤§å°çš„è¾“å…¥ã€‚

```python
# åˆ›å»ºè¾“å…¥ç¤ºä¾‹
batch_size = 1
input_channels = 3
input_height = 32
input_width = 32
input_tensor = torch.randn(batch_size, input_channels, input_height, input_width)

# æŒ‡å®šåŠ¨æ€è½´
dynamic_axes = {
    'input': {0: 'batch_size'},
    'output': {0: 'batch_size'}
}
```

### 4. å¯¼å‡ºæ¨¡å‹åˆ°ONNX

ä½¿ç”¨ `torch.onnx.export`å‡½æ•°å°†PyTorchæ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼ã€‚éœ€è¦æŒ‡å®šæ¨¡å‹ã€è¾“å…¥ç¤ºä¾‹ã€è¾“å‡ºæ–‡ä»¶åã€åŠ¨æ€è½´ç­‰å‚æ•°ã€‚

```python
# å¯¼å‡ºæ¨¡å‹
output_file ='model.onnx'
torch.onnx.export(
    model,
    input_tensor,
    output_file,
    export_params=True,
    opset_version=11,
    do_estimation=True,
    dynamic_axes=dynamic_axes
)
```

ä¸Šè¿°ä»£ç ä¸­ï¼Œ`export_params=True`è¡¨ç¤ºå°†æ¨¡å‹çš„å‚æ•°ä¸€èµ·å¯¼å‡ºï¼Œ`opset_version=11`æŒ‡å®šäº†ONNXçš„æ“ä½œé›†ç‰ˆæœ¬ï¼Œ`do_estimation=True`ä¼šå°è¯•ä¼˜åŒ–æ¨¡å‹ï¼Œ`dynamic_axes`æŒ‡å®šäº†åŠ¨æ€è½´ã€‚

åœ¨è½¬æ¢è¿‡ç¨‹ä¸­ï¼Œå¯èƒ½ä¼šé‡åˆ°ä¸€äº›æ¨¡å‹ä¸æ”¯æŒçš„æ“ä½œæˆ–æ•°æ®ç±»å‹ç­‰é—®é¢˜ã€‚è¿™æ—¶éœ€è¦æ£€æŸ¥æ¨¡å‹çš„ä»£ç å’Œç»“æ„ï¼Œç¡®ä¿æ¨¡å‹ä¸­çš„æ“ä½œéƒ½èƒ½è¢«ONNXæ”¯æŒã€‚å¯èƒ½éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œä¸€äº›ä¿®æ”¹ï¼Œå¦‚å°†ä¸€äº›è‡ªå®šä¹‰çš„æ“ä½œè½¬æ¢ä¸ºONNXæ”¯æŒçš„æ ‡å‡†æ“ä½œç­‰ã€‚

## Onnxrunime æ¨ç†éªŒè¯

ä½¿ç”¨ONNX Runtimeè¿›è¡Œæ¨ç†éªŒè¯ï¼Œä¸»è¦åŒ…æ‹¬å®‰è£…ONNX Runtimeã€åŠ è½½ONNXæ¨¡å‹ã€å‡†å¤‡è¾“å…¥æ•°æ®ã€è¿›è¡Œæ¨ç†åŠéªŒè¯ç»“æœç­‰æ­¥éª¤ï¼Œä»¥ä¸‹æ˜¯è¯¦ç»†è¯´æ˜åŠä»£ç ç¤ºä¾‹ï¼š

### 1. å®‰è£…ONNX Runtime

å¯ä»¥ä½¿ç”¨ `pip`å‘½ä»¤å®‰è£…ONNX Runtimeï¼Œæ ¹æ®ä½¿ç”¨çš„å¹³å°å’Œéœ€æ±‚é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬ã€‚

- **CPUç‰ˆæœ¬**ï¼šé€‚ç”¨äºæ²¡æœ‰GPUæˆ–å¯¹GPUåŠ é€Ÿéœ€æ±‚ä¸é«˜çš„åœºæ™¯ã€‚

```
pip install onnxruntime
```

- **GPUç‰ˆæœ¬**ï¼šéœ€è¦ç³»ç»Ÿå®‰è£…äº†ç›¸åº”çš„NVIDIAæ˜¾å¡é©±åŠ¨å’ŒCUDAç­‰ç¯å¢ƒï¼Œèƒ½åˆ©ç”¨GPUåŠ é€Ÿæ¨ç†ã€‚

```
pip install onnxruntime-gpu
```

### 2. åŠ è½½ONNXæ¨¡å‹

ä½¿ç”¨ONNX RuntimeåŠ è½½å·²ç»è½¬æ¢å¥½çš„ONNXæ¨¡å‹ã€‚

```python
import onnxruntime as ort

# åˆ›å»ºONNX Runtimeæ¨ç†ä¼šè¯
ort_session = ort.InferenceSession("model.onnx")
```

### 3. å‡†å¤‡è¾“å…¥æ•°æ®

å‡†å¤‡ä¸æ¨¡å‹è¾“å…¥æ ¼å¼å’Œå½¢çŠ¶åŒ¹é…çš„æ•°æ®ã€‚æ•°æ®å¯ä»¥æ˜¯éšæœºç”Ÿæˆçš„ï¼Œä¹Ÿå¯ä»¥æ˜¯çœŸå®çš„æ ·æœ¬æ•°æ®ã€‚

```python
import numpy as np

# åˆ›å»ºä¸æ¨¡å‹è¾“å…¥å½¢çŠ¶åŒ¹é…çš„éšæœºæ•°æ®
input_shape = (1, 3, 32, 32)  # æ ¹æ®æ¨¡å‹å®é™…è¾“å…¥å½¢çŠ¶è°ƒæ•´
input_data = np.random.randn(*input_shape).astype(np.float32)

# åˆ›å»ºè¾“å…¥å­—å…¸ï¼Œé”®ä¸ºæ¨¡å‹è¾“å…¥åç§°ï¼Œå€¼ä¸ºè¾“å…¥æ•°æ®
input_name = ort_session.get_inputs()[0].name
inputs = {input_name: input_data}
```

### 4. è¿›è¡Œæ¨ç†

ä½¿ç”¨åŠ è½½çš„æ¨¡å‹å’Œå‡†å¤‡å¥½çš„è¾“å…¥æ•°æ®è¿›è¡Œæ¨ç†ã€‚

```python
# è¿è¡Œæ¨ç†
outputs = ort_session.run(None, inputs)
```

### 5. éªŒè¯ç»“æœ

å¯ä»¥æ ¹æ®å…·ä½“ä»»åŠ¡å’Œéœ€æ±‚ï¼Œé‡‡ç”¨ä¸åŒçš„æ–¹å¼éªŒè¯æ¨ç†ç»“æœã€‚æ¯”å¦‚å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œå¯ä»¥æ£€æŸ¥è¾“å‡ºçš„ç±»åˆ«æ¦‚ç‡æ˜¯å¦åˆç†ï¼›å¯¹äºå›å½’ä»»åŠ¡ï¼Œå¯ä»¥è®¡ç®—é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„è¯¯å·®ç­‰ã€‚

```python
# å‡è®¾æ˜¯åˆ†ç±»ä»»åŠ¡ï¼Œè¾“å‡ºç»“æœæ˜¯ç±»åˆ«æ¦‚ç‡ï¼Œæ‰“å°æœ€é«˜æ¦‚ç‡çš„ç±»åˆ«
predicted_class = np.argmax(outputs[0], axis=1)
print("Predicted Class:", predicted_class)
```

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯èƒ½è¿˜éœ€è¦å¤„ç†æ¨¡å‹æœ‰å¤šä¸ªè¾“å…¥å’Œè¾“å‡ºçš„æƒ…å†µï¼Œä»¥åŠå¯¹æ¨ç†ç»“æœè¿›è¡Œæ›´å¤æ‚çš„è¯„ä¼°å’Œåˆ†æç­‰ã€‚æ­¤å¤–ï¼Œå¦‚æœåœ¨æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œå¦‚æ¨¡å‹ä¸æ”¯æŒçš„æ“ä½œã€æ•°æ®ç±»å‹ä¸åŒ¹é…ç­‰ï¼Œéœ€è¦æ ¹æ®é”™è¯¯ä¿¡æ¯æ£€æŸ¥æ¨¡å‹å’Œè¾“å…¥æ•°æ®ï¼Œè¿›è¡Œç›¸åº”çš„è°ƒæ•´å’Œä¿®å¤ã€‚

## ONNXè½¬TensorRT

å°†ONNXæ¨¡å‹è½¬æ¢ä¸ºTensorRTæ¨¡å‹ï¼Œä¸€èˆ¬éœ€è¦å®‰è£…TensorRTã€åŠ è½½ONNXæ¨¡å‹ã€åˆ›å»ºTensorRTæ„å»ºå™¨ç­‰æ­¥éª¤ï¼Œä»¥ä¸‹æ˜¯è¯¦ç»†çš„æ­¥éª¤å’Œä»£ç ç¤ºä¾‹ï¼š

### 1. å®‰è£…TensorRTå’Œç›¸å…³ä¾èµ–

- é¦–å…ˆéœ€è¦å®‰è£…TensorRTï¼Œå¯ä»¥ä»NVIDIAå®˜æ–¹ç½‘ç«™ä¸‹è½½é€‚åˆè‡ªå·±ç³»ç»Ÿå’ŒCUDAç‰ˆæœ¬çš„TensorRTå®‰è£…åŒ…è¿›è¡Œå®‰è£…ã€‚
- åŒæ—¶ï¼Œç¡®ä¿å®‰è£…äº†Pythonçš„TensorRTç»‘å®šåº“ï¼Œå¯ä»¥ä½¿ç”¨ `pip`å®‰è£…ï¼š

```
pip install nvidia-tensorrt
```

- è¿˜éœ€è¦å®‰è£… `onnx-tensorrt`æ’ä»¶ï¼Œå®ƒæ˜¯ONNXå’ŒTensorRTä¹‹é—´çš„æ¡¥æ¢ï¼š

```
pip install onnx-tensorrt
```

### 2. åŠ è½½ONNXæ¨¡å‹

ä½¿ç”¨Pythonçš„ `onnx`åº“åŠ è½½ONNXæ¨¡å‹ã€‚

```python
import onnx

# åŠ è½½ONNXæ¨¡å‹
onnx_model = onnx.load("model.onnx")
```

### 3. åˆ›å»ºTensorRTæ„å»ºå™¨å’Œç½‘ç»œå®šä¹‰

ä½¿ç”¨TensorRTçš„Python APIåˆ›å»ºæ„å»ºå™¨å’Œç½‘ç»œå®šä¹‰ã€‚

```python
import tensorrt as trt

# åˆ›å»ºTensorRTæ—¥å¿—è®°å½•å™¨
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)

# åˆ›å»ºTensorRTæ„å»ºå™¨
builder = trt.Builder(TRT_LOGGER)

# åˆ›å»ºTensorRTç½‘ç»œå®šä¹‰
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))

# åˆ›å»ºONNXè§£æå™¨
parser = trt.OnnxParser(network, TRT_LOGGER)

# è§£æONNXæ¨¡å‹
if not parser.parse(onnx_model.SerializeToString()):
    print("Error parsing ONNX model")
    for error in range(parser.num_errors):
        print(parser.get_error(error))
```

### 4. è®¾ç½®TensorRTæ„å»ºå™¨é…ç½®

é…ç½®æ„å»ºå™¨ï¼Œä¾‹å¦‚è®¾ç½®æœ€å¤§æ‰¹å¤„ç†å¤§å°ã€å·¥ä½œç©ºé—´å¤§å°ç­‰å‚æ•°ã€‚

```python
# è®¾ç½®æœ€å¤§æ‰¹å¤„ç†å¤§å°
builder.max_batch_size = 1

# è®¾ç½®å·¥ä½œç©ºé—´å¤§å°ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰
builder.max_workspace_size = 1 << 30  # 1GB

# å¯ç”¨FP16æ¨¡å¼ï¼ˆå¦‚æœç¡¬ä»¶æ”¯æŒä¸”éœ€è¦ï¼‰
if builder.platform_has_fast_fp16:
    builder.fp16_mode = True
```

### 5. æ„å»ºTensorRTå¼•æ“

ä½¿ç”¨é…ç½®å¥½çš„æ„å»ºå™¨å’Œç½‘ç»œå®šä¹‰æ„å»ºTensorRTå¼•æ“ã€‚

```python
# æ„å»ºTensorRTå¼•æ“
engine = builder.build_cuda_engine(network)
```

### 6. ä¿å­˜TensorRTå¼•æ“

å°†æ„å»ºå¥½çš„TensorRTå¼•æ“ä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨ã€‚

```python
import os

# ä¿å­˜TensorRTå¼•æ“åˆ°æ–‡ä»¶
def save_engine(engine, engine_file_path):
    with open(engine_file_path, "wb") as f:
        f.write(engine.serialize())

# æŒ‡å®šä¿å­˜å¼•æ“çš„æ–‡ä»¶è·¯å¾„
engine_file_path = "model.engine"
save_engine(engine, engine_file_path)
```

ä¸Šè¿°ä»£ç ä¸­ï¼Œé€šè¿‡ä¸€ç³»åˆ—çš„æ“ä½œå°†ONNXæ¨¡å‹è½¬æ¢ä¸ºTensorRTå¼•æ“ï¼Œå¹¶ä¿å­˜ä¸º `.engine`æ–‡ä»¶ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯èƒ½éœ€è¦æ ¹æ®æ¨¡å‹çš„ç‰¹ç‚¹å’Œéœ€æ±‚ï¼Œè¿›ä¸€æ­¥è°ƒæ•´æ„å»ºå™¨çš„é…ç½®å‚æ•°ï¼Œä»¥ä¼˜åŒ–TensorRTå¼•æ“çš„æ€§èƒ½ã€‚

## Tensort æ¨ç†

TensorRTæ¨¡å‹æ¨ç†ä¸€èˆ¬åŒ…æ‹¬åŠ è½½TensorRTå¼•æ“ã€å‡†å¤‡è¾“å…¥æ•°æ®ã€æ‰§è¡Œæ¨ç†ã€å¤„ç†è¾“å‡ºç»“æœç­‰æ­¥éª¤ï¼Œä»¥ä¸‹æ˜¯è¯¦ç»†çš„æ­¥éª¤å’Œä»£ç ç¤ºä¾‹ï¼š

### 1. å¯¼å…¥å¿…è¦çš„åº“

å¯¼å…¥TensorRTã€PyCUDAç­‰ç›¸å…³åº“ï¼Œç”¨äºæ¨¡å‹æ¨ç†å’ŒCUDAæ“ä½œã€‚

```python
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np
```

### 2. åŠ è½½TensorRTå¼•æ“

ä»ä¹‹å‰ä¿å­˜çš„ `.engine`æ–‡ä»¶ä¸­åŠ è½½TensorRTå¼•æ“ï¼Œå¹¶åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡ã€‚

```python
# åŠ è½½TensorRTå¼•æ“
def load_engine(engine_file_path):
    with open(engine_file_path, "rb") as f, trt.Runtime(trt.Logger(trt.Logger.WARNING)) as runtime:
        return runtime.deserialize_cuda_engine(f.read())

# åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
engine_file_path = "model.engine"
engine = load_engine(engine_file_path)
context = engine.create_execution_context()
```

### 3. å‡†å¤‡è¾“å…¥æ•°æ®

æ ¹æ®æ¨¡å‹çš„è¾“å…¥è¦æ±‚ï¼Œå‡†å¤‡è¾“å…¥æ•°æ®ã€‚å°†æ•°æ®è½¬æ¢ä¸ºåˆé€‚çš„æ ¼å¼ï¼Œå¹¶åˆ†é…GPUå†…å­˜ã€‚

```python
# å‡è®¾è¾“å…¥æ•°æ®æ˜¯å½¢çŠ¶ä¸º(1, 3, 224, 224)çš„å›¾åƒæ•°æ®ï¼Œæ•°æ®ç±»å‹ä¸ºfloat32
input_shape = (1, 3, 224, 224)
input_data = np.random.randn(*input_shape).astype(np.float32)

# ä¸ºè¾“å…¥æ•°æ®åˆ†é…GPUå†…å­˜
input_buffer = cuda.mem_alloc(input_data.nbytes)
cuda.memcpy_htod(input_buffer, input_data)
```

### 4. å‡†å¤‡è¾“å‡ºå†…å­˜

æ ¹æ®æ¨¡å‹çš„è¾“å‡ºè¦æ±‚ï¼Œåˆ†é…è¾“å‡ºå†…å­˜ã€‚

```python
# è·å–è¾“å‡ºå¼ é‡çš„å½¢çŠ¶å’Œæ•°æ®ç±»å‹
output_shape = (1, 1000)  # æ ¹æ®å®é™…æ¨¡å‹è¾“å‡ºå½¢çŠ¶è°ƒæ•´
output_dtype = np.float32

# ä¸ºè¾“å‡ºæ•°æ®åˆ†é…GPUå†…å­˜
output_data = np.empty(output_shape, dtype=output_dtype)
output_buffer = cuda.mem_alloc(output_data.nbytes)
```

### 5. æ‰§è¡Œæ¨ç†

å°†è¾“å…¥æ•°æ®ä¼ é€’ç»™æ¨¡å‹ï¼Œæ‰§è¡Œæ¨ç†ï¼Œå¹¶å°†è¾“å‡ºç»“æœå­˜å‚¨åˆ°è¾“å‡ºå†…å­˜ä¸­ã€‚

```python
# åˆ›å»ºç»‘å®šç¼“å†²åŒºåˆ—è¡¨ï¼ŒåŒ…å«è¾“å…¥å’Œè¾“å‡ºç¼“å†²åŒº
bindings = [int(input_buffer), int(output_buffer)]

# æ‰§è¡Œæ¨ç†
context.execute_v2(bindings)
```

### 6. å¤„ç†è¾“å‡ºç»“æœ

å°†è¾“å‡ºç»“æœä»GPUå†…å­˜å¤åˆ¶åˆ°CPUå†…å­˜ï¼Œå¹¶è¿›è¡Œåç»­å¤„ç†ï¼Œå¦‚æ‰“å°ç»“æœæˆ–è¿›è¡Œè¯„ä¼°ã€‚

```python
# å°†è¾“å‡ºç»“æœä»GPUå¤åˆ¶åˆ°CPU
cuda.memcpy_dtoh(output_data, output_buffer)

# å¤„ç†è¾“å‡ºç»“æœï¼Œä¾‹å¦‚æ‰“å°ç±»åˆ«æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«
predicted_class = np.argmax(output_data, axis=1)
print("Predicted Class:", predicted_class)
```

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„æ¨¡å‹å’Œä»»åŠ¡å¯¹ä»£ç è¿›è¡Œè°ƒæ•´ï¼Œè¿˜å¯ä»¥æ·»åŠ é”™è¯¯å¤„ç†ã€æ€§èƒ½ä¼˜åŒ–ç­‰åŠŸèƒ½ï¼Œä»¥æé«˜æ¨ç†çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

## tensorrt C++ éƒ¨ç½²æ¨ç†

ä»¥ä¸‹æ˜¯åœ¨C++ä¸­ä½¿ç”¨TensorRTè¿›è¡Œæ¨¡å‹éƒ¨ç½²æ¨ç†çš„è¯¦ç»†æ­¥éª¤ï¼ŒåŒ…å«äº†ä»åŠ è½½æ¨¡å‹å¼•æ“åˆ°æ‰§è¡Œæ¨ç†ä»¥åŠå¤„ç†ç»“æœçš„å®Œæ•´æµç¨‹ï¼š

### 1. åŒ…å«å¿…è¦çš„å¤´æ–‡ä»¶å’Œé“¾æ¥ç›¸å…³åº“

åœ¨C++ä»£ç ä¸­ï¼Œé¦–å…ˆéœ€è¦åŒ…å«TensorRTç›¸å…³çš„å¤´æ–‡ä»¶ä»¥åŠCUDAçš„å¤´æ–‡ä»¶ï¼ˆå› ä¸ºTensorRTä¾èµ–äºCUDAè¿›è¡ŒGPUåŠ é€Ÿï¼‰ï¼ŒåŒæ—¶è¦ç¡®ä¿åœ¨ç¼–è¯‘æ—¶é“¾æ¥å¯¹åº”çš„åº“æ–‡ä»¶ã€‚

```cpp
#include <iostream>
#include <fstream>
#include <vector>
#include <NvInfer.h>
#include <cuda_runtime.h>

// å¼•å…¥å¿…è¦çš„å‘½åç©ºé—´
using namespace nvinfer1;
using namespace std;
```

### 2. åŠ è½½TensorRTå¼•æ“

ä»ç£ç›˜ä¸Šä¿å­˜çš„ `.engine`æ–‡ä»¶ä¸­åŠ è½½TensorRTå¼•æ“ï¼Œè¿™æ˜¯æ‰§è¡Œæ¨ç†çš„æ ¸å¿ƒç»„ä»¶ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªåŠ è½½å¼•æ“çš„å‡½æ•°ç¤ºä¾‹ï¼š

```cpp
// å‡½æ•°ç”¨äºä»æ–‡ä»¶ä¸­è¯»å–å¹¶ååºåˆ—åŒ–TensorRTå¼•æ“
ICudaEngine* loadEngine(const string& engineFilePath) {
    ifstream file(engineFilePath, ios::binary);
    if (!file) {
        cerr << "Failed to open engine file: " << engineFilePath << endl;
        return nullptr;
    }
    file.seekg(0, ios::end);
    size_t size = file.tellg();
    file.seekg(0, ios::beg);
    char* buffer = new char[size];
    file.read(buffer, size);
    file.close();

    IRuntime* runtime = createInferRuntime(gLogger);
    ICudaEngine* engine = runtime->deserializeCudaEngine(buffer, size);
    delete[] buffer;
    return engine;
}
```

### 3. å‡†å¤‡è¾“å…¥è¾“å‡ºæ•°æ®ç»“æ„

éœ€è¦æ ¹æ®æ¨¡å‹çš„è¾“å…¥è¾“å‡ºè¦æ±‚ï¼Œå®šä¹‰ç›¸åº”çš„æ•°æ®ç»“æ„ï¼ŒåŒ…æ‹¬åœ¨GPUä¸Šåˆ†é…å†…å­˜æ¥å­˜å‚¨æ•°æ®ï¼Œå¹¶åˆ›å»ºå¯¹åº”çš„CUDAæµç­‰ã€‚

```cpp
// ç»“æ„ä½“ç”¨äºç®¡ç†è¾“å…¥è¾“å‡ºç¼“å†²åŒº
struct BufferManager {
    vector<void*> deviceBuffers;
    vector<void*> hostBuffers;
    int numBuffers;
    BufferManager(int numBuffers_) : numBuffers(numBuffers_) {
        deviceBuffers.resize(numBuffers);
        hostBuffers.resize(numBuffers);
    }

    // åœ¨GPUå’ŒCPUä¸Šåˆ†é…å†…å­˜
    void allocateBuffers(ICudaEngine* engine) {
        for (int i = 0; i < numBuffers; ++i) {
            Dims dims = engine->getBindingDimensions(i);
            size_t size = getSizeByDim(dims) * sizeof(float);
            cudaMalloc(&deviceBuffers[i], size);
            hostBuffers[i] = new float[getSizeByDim(dims)];
        }
    }

    // é‡Šæ”¾å†…å­˜
    void freeBuffers() {
        for (int i = 0; i < numBuffers; ++i) {
            cudaFree(deviceBuffers[i]);
            delete[] hostBuffers[i];
        }
    }

private:
    // æ ¹æ®ç»´åº¦è®¡ç®—å…ƒç´ ä¸ªæ•°
    int getSizeByDim(const Dims& dims) {
        int size = 1;
        for (int i = 0; i < dims.nbDims; ++i) {
            size *= dims.d[i];
        }
        return size;
    }
};
```

### 4. æ‰§è¡Œæ¨ç†

åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œå°†è¾“å…¥æ•°æ®ä»CPUå¤åˆ¶åˆ°GPUï¼Œç„¶åæ‰§è¡Œæ¨ç†æ“ä½œï¼Œæœ€åå°†è¾“å‡ºç»“æœä»GPUå¤åˆ¶å›CPUã€‚

```cpp
// æ‰§è¡ŒTensorRTæ¨ç†çš„å‡½æ•°
void doInference(ICudaEngine* engine, BufferManager& buffers) {
    IExecutionContext* context = engine->createExecutionContext();
    if (!context) {
        cerr << "Failed to create execution context" << endl;
        return;
    }

    // å°†è¾“å…¥æ•°æ®ä»CPUå¤åˆ¶åˆ°GPU
    for (int i = 0; i < engine->getNbBindings(); ++i) {
        if (engine->bindingIsInput(i)) {
            cudaMemcpy(buffers.deviceBuffers[i], buffers.hostBuffers[i],
                       getSizeByDim(engine->getBindingDimensions(i)) * sizeof(float),
                       cudaMemcpyHostToDevice);
        }
    }

    // æ‰§è¡Œæ¨ç†
    context->executeV2(buffers.deviceBuffers.data());

    // å°†è¾“å‡ºç»“æœä»GPUå¤åˆ¶å›CPU
    for (int i = 0; i < engine->getNbBindings(); ++i) {
        if (!engine->bindingIsInput(i)) {
            cudaMemcpy(buffers.hostBuffers[i], buffers.deviceBuffers[i],
                       getSizeByDim(engine->getBindingDimensions(i)) * sizeof(float),
                       cudaMemcpyDeviceToHost);
        }
    }

    context->destroy();
}
```

### 5. ä¸»å‡½æ•°æ•´åˆåŠè°ƒç”¨

åœ¨ä¸»å‡½æ•°ä¸­ï¼Œè°ƒç”¨ä¸Šè¿°çš„å„ä¸ªå‡½æ•°æ¥å®Œæˆæ•´ä¸ªTensorRTæ¨¡å‹çš„éƒ¨ç½²æ¨ç†æµç¨‹ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

```cpp
int main() {
    // åŠ è½½TensorRTå¼•æ“
    string engineFilePath = "model.engine";
    ICudaEngine* engine = loadEngine(engineFilePath);
    if (!engine) {
        return -1;
    }

    // åˆ›å»ºç¼“å†²åŒºç®¡ç†å™¨å¹¶åˆ†é…å†…å­˜
    BufferManager buffers(engine->getNbBindings());
    buffers.allocateBuffers(engine);

    // å‡è®¾è¿™é‡Œç®€å•åœ°å¡«å……è¾“å…¥æ•°æ®ï¼ˆå®é™…ä¸­æ ¹æ®å…·ä½“æ¨¡å‹è¾“å…¥è¦æ±‚å¡«å……ï¼‰
    fillInputData(buffers.hostBuffers[0]);

    // æ‰§è¡Œæ¨ç†
    doInference(engine, buffers);

    // å¤„ç†è¾“å‡ºç»“æœï¼ˆæ ¹æ®å…·ä½“ä»»åŠ¡å¤„ç†ï¼‰
    processOutputData(buffers.hostBuffers[engine->getNbBindings() - 1]);

    // é‡Šæ”¾å†…å­˜
    buffers.freeBuffers();
    engine->destroy();

    return 0;
}
```

åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼š

- `fillInputData`å‡½æ•°ç”¨äºæ ¹æ®æ¨¡å‹çš„å®é™…è¾“å…¥è¦æ±‚ï¼Œå‘ `hostBuffers`ä¸­å¯¹åº”çš„è¾“å…¥ç¼“å†²åŒºå¡«å……åˆé€‚çš„æ•°æ®ï¼Œè¿™éƒ¨åˆ†ä»£ç éœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹çš„è¾“å…¥ç»´åº¦ã€æ•°æ®ç±»å‹ç­‰è¿›è¡Œç¼–å†™ã€‚
- `processOutputData`å‡½æ•°åˆ™æ˜¯æŒ‰ç…§å…·ä½“çš„ä»»åŠ¡ç±»å‹ï¼ˆæ¯”å¦‚åˆ†ç±»ä»»åŠ¡æŸ¥çœ‹é¢„æµ‹ç±»åˆ«ã€å›å½’ä»»åŠ¡åˆ†æé¢„æµ‹æ•°å€¼ç­‰ï¼‰å¯¹è¾“å‡ºç¼“å†²åŒºä¸­çš„æ•°æ®è¿›è¡Œå¤„ç†ï¼ŒåŒæ ·éœ€è¦ä¾æ®å…·ä½“åº”ç”¨åœºæ™¯æ¥å®ç°ã€‚

å¦å¤–ï¼Œåœ¨å®é™…ç¼–è¯‘è¿è¡Œè¿™æ®µä»£ç æ—¶ï¼Œéœ€è¦æ­£ç¡®é…ç½®ç¼–è¯‘ç¯å¢ƒï¼ŒåŒ…å«TensorRTåº“å’ŒCUDAåº“çš„è·¯å¾„ä»¥åŠé“¾æ¥ç›¸åº”çš„åº“æ–‡ä»¶ï¼Œä¸åŒçš„å¼€å‘ç¯å¢ƒï¼ˆå¦‚Visual Studioã€GCCç­‰ï¼‰é…ç½®æ–¹å¼ä¼šæœ‰æ‰€ä¸åŒã€‚


## TensorRT C++éƒ¨ç½²

### onnxæ¨¡å‹è½¬engine

- **åŸºäºC++ä»£ç ç”Ÿæˆengine**ï¼šæ„å»º `IBuilder`ã€`INetworkDefinition`ã€`nvonnxparser::IParser`ç­‰å¯¹è±¡ï¼Œé€šè¿‡ `parser`è§£æonnxæ–‡ä»¶ï¼Œåˆ©ç”¨ `builder`å’Œé…ç½®å¯¹è±¡ `IBuilderConfig`åˆ›å»ºæ¨ç†å¼•æ“ `ICudaEngine`ï¼Œæœ€åå°† `engine`åºåˆ—åŒ–å¹¶ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ã€‚
- **åŸºäºtrtexec.exeå‘½ä»¤è¡Œç”Ÿæˆ**ï¼šå¯ä½¿ç”¨ `trtexec.exe`å‘½ä»¤è¡Œå·¥å…·ï¼ŒæŒ‡å®š `--onnx`å‚æ•°ä¸ºonnxæ–‡ä»¶è·¯å¾„ï¼Œ`--saveEngine`å‚æ•°ä¸ºä¿å­˜engineæ–‡ä»¶è·¯å¾„ï¼Œå®ç°å°†onnxæ¨¡å‹è½¬æ¢ä¸ºengineã€‚

### è¯»å–æœ¬åœ°æ¨¡å‹

ä½¿ç”¨ `std::ifstream`ä»¥äºŒè¿›åˆ¶æ–¹å¼æ‰“å¼€engineæ–‡ä»¶ï¼Œå°†æ–‡ä»¶å†…å®¹è¯»å–åˆ°å†…å­˜ä¸­ã€‚å¯ä»¥å®šä¹‰ä¸€ä¸ªå­—ç¬¦æŒ‡é’ˆå’Œæ–‡ä»¶å¤§å°å˜é‡ï¼Œé€šè¿‡ç§»åŠ¨æ–‡ä»¶æŒ‡é’ˆè·å–æ–‡ä»¶å¤§å°ï¼Œå†å°†æ–‡ä»¶æŒ‡é’ˆç§»å›æ–‡ä»¶å¼€å§‹å¤„ï¼Œè¯»å–æ–‡ä»¶æ•°æ®åˆ°å­—ç¬¦æŒ‡é’ˆæŒ‡å‘çš„å†…å­˜åŒºåŸŸã€‚

### åˆ›å»ºæ¨ç†å¼•æ“

åˆ›å»º `nvinfer1::IRuntime`å¯¹è±¡ï¼Œé€šè¿‡ `createInferRuntime`å‡½æ•°å®ç°ã€‚ç„¶åä½¿ç”¨ `runtime`çš„ `deserializeCudaEngine`å‡½æ•°ï¼Œä¼ å…¥ä¹‹å‰è¯»å–çš„engineæ–‡ä»¶æ•°æ®å’Œæ–‡ä»¶å¤§å°ï¼Œåˆ›å»ºæ¨ç†å¼•æ“ `ICudaEngine`ã€‚

### åˆ›å»ºæ¨ç†ä¸Šä¸‹æ–‡

é€šè¿‡æ¨ç†å¼•æ“ `ICudaEngine`çš„ `createExecutionContext`å‡½æ•°åˆ›å»ºæ¨ç†ä¸Šä¸‹æ–‡ `IExecutionContext`ï¼Œè¯¥ä¸Šä¸‹æ–‡ç”¨äºæ‰§è¡Œæ¨ç†æ“ä½œã€‚

### åˆ›å»ºGPUæ˜¾å­˜ç¼“å†²åŒº

æ ¹æ®æ¨¡å‹çš„è¾“å…¥è¾“å‡ºä¿¡æ¯ï¼Œè®¡ç®—æ‰€éœ€çš„æ˜¾å­˜å¤§å°ã€‚ä½¿ç”¨ `cudaMalloc`å‡½æ•°åœ¨GPUä¸Šåˆ†é…è¾“å…¥å’Œè¾“å‡ºç¼“å†²åŒºçš„æ˜¾å­˜ç©ºé—´ã€‚å°†åˆ†é…çš„æ˜¾å­˜æŒ‡é’ˆä¸æ¨¡å‹çš„è¾“å…¥è¾“å‡ºç»‘å®šï¼Œå¯é€šè¿‡ `IExecutionContext`çš„ `setBindingAddress`å‡½æ•°å®ç°ã€‚

### é…ç½®è¾“å…¥æ•°æ®

å°†è¾“å…¥æ•°æ®ä»ä¸»æœºå†…å­˜å¤åˆ¶åˆ°GPUæ˜¾å­˜çš„è¾“å…¥ç¼“å†²åŒºä¸­ï¼Œå¯ä½¿ç”¨ `cudaMemcpy`å‡½æ•°å®ç°ã€‚è®¾ç½®è¾“å…¥æ•°æ®çš„ç»´åº¦ã€æ•°æ®ç±»å‹ç­‰ä¿¡æ¯ï¼Œç¡®ä¿ä¸æ¨¡å‹çš„è¾“å…¥è¦æ±‚åŒ¹é…ã€‚

### æ¨¡å‹æ¨ç†

ä½¿ç”¨ `IExecutionContext`çš„ `execute`æˆ– `executeV2`å‡½æ•°æ‰§è¡Œæ¨ç†æ“ä½œï¼Œä¼ å…¥è¾“å…¥è¾“å‡ºç¼“å†²åŒºçš„ç»‘å®šä¿¡æ¯ï¼Œè®©æ¨¡å‹åœ¨GPUä¸Šè¿›è¡Œæ¨ç†è®¡ç®—ã€‚

### è·å¾—è¾“å‡ºæ•°æ®

å°†æ¨ç†ç»“æœä»GPUæ˜¾å­˜çš„è¾“å‡ºç¼“å†²åŒºå¤åˆ¶åˆ°ä¸»æœºå†…å­˜ä¸­ï¼Œä½¿ç”¨ `cudaMemcpy`å‡½æ•°å®ç°ã€‚å¯¹è¾“å‡ºæ•°æ®è¿›è¡Œåå¤„ç†ï¼Œå¦‚è§£ææ£€æµ‹ç»“æœã€è¿›è¡Œåˆ†ç±»åˆ¤æ–­ã€ç»˜åˆ¶æ£€æµ‹æ¡†ç­‰ï¼Œæ ¹æ®å…·ä½“çš„æ¨¡å‹å’Œåº”ç”¨éœ€æ±‚è¿›è¡Œç›¸åº”çš„å¤„ç†ã€‚

[tensorrt step transform tutorials](TensorRT%2005f7cab57cf84b489ed279fe7a335852/tensorrt%20step%20transform%20tutorials%20e0c7d04f3e1d41d2be67d69107a92f57.md)
